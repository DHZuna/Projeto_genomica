{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3173e5ff-06a3-4ecc-8ee3-45ab179cfbcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# No Terminal\n",
    "\n",
    "* pip install --upgrade pip\n",
    "* pip install kafka-python\n",
    "* pip install newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0c144c-08be-488e-8ca7-2591b7c64690",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b9ea19-bd51-4fff-82a5-0bf1d6d5b054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from kafka import KafkaConsumer\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab469702-5575-4952-9a35-7e2f84077144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8700e25f-ad57-42f4-a4d0-3323a41d96f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Data_Pipeline_API:\n",
    "    def __init__(self) -> None:\n",
    "        self.__df = None\n",
    "        self.__df_parquet = None\n",
    "\n",
    "    \n",
    "    def load_api(self, data) -> None:\n",
    "        self.__df = ps.DataFrame(data)\n",
    "        sources = []\n",
    "        for aux in self.__df['source'].to_numpy():\n",
    "            if isinstance(aux, dict):\n",
    "                sources.append(aux['name'])\n",
    "            else:\n",
    "                sources.append(None)\n",
    "        self.__df['source'] = sources\n",
    "\n",
    "\n",
    "    # Transformar os dados .parquet em DataFrame\n",
    "    def load_parquet(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        self.__df_parquet = ps.read_parquet(f\"/FileStore/Projeto/data/{file}\")\n",
    "\n",
    "\n",
    "    # Concatenando o DataFrame com o arquivo parquet\n",
    "    # Mantém apenas colunas únicas\n",
    "    def save_data(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        if file in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")] or f\"{file}/\" in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")]:\n",
    "            self.load_parquet(name)\n",
    "            self.__df = ps.concat([self.__df_parquet, self.__df]).drop_duplicates().reset_index().drop(columns=[\"index\"])\n",
    "        self.__df.to_parquet(f\"/FileStore/Projeto/data/{file}\", mode='overwrite')\n",
    "\n",
    "\n",
    "    def data_cleaning(self):\n",
    "        # Remove coluna de url de imagem de capa\n",
    "        self.__df = self.__df_parquet.drop(columns=['urlToImage'])\n",
    "        \n",
    "        #Alterar o nome das colunas\n",
    "        self.__df = self.__df.rename(columns={'author': 'Autor',\n",
    "                                              'title': 'Título',\n",
    "                                              'description': 'Descrição',\n",
    "                                              'url' : 'URL',\n",
    "                                              'publishedAt': 'Data Publicação',\n",
    "                                              'content': 'Conteúdo',\n",
    "                                              'source': 'Fonte'})\n",
    "\n",
    "        # Organizar colunas\n",
    "        df_order = ['Data Publicação', 'Título', 'Autor', 'Descrição', 'URL', 'Fonte', 'Conteúdo']\n",
    "        self.__df = self.__df[df_order]\n",
    "\n",
    "        # Ajuste do dia de publicação\n",
    "        data = self.__df['Data Publicação'].to_numpy()\n",
    "        date_df = pd.DataFrame(data = data)\n",
    "        date_df.columns = ['Data Publicação']\n",
    "        date_df['Data Publicação'] = pd.to_datetime(date_df['Data Publicação'])\n",
    "        date_df['Data Publicação'] = date_df['Data Publicação'].dt.strftime('%Y/%m/%d')\n",
    "\n",
    "        for i in range(self.__df.shape[0]):\n",
    "            self.__df.loc[i, 'Data Publicação'] = date_df.loc[i, 'Data Publicação']\n",
    "\n",
    "        # Ordenar Datas\n",
    "        self.__df = self.__df.sort_values('Data Publicação').reset_index().drop(columns=['index'])\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self.__df\n",
    "\n",
    "\n",
    "    @property\n",
    "    def df_parquet(self):\n",
    "        return self.__df_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1836f941-4907-48e5-bbb0-66aa4bf92960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478117f3-fd32-4d41-b93c-5585a3785b51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\"genomics-news\", bootstrap_servers=[\"localhost:9092\"])\n",
    "\n",
    "for message in consumer:\n",
    "    value = message.value\n",
    "    articles_list = pickle.loads(value)\n",
    "\n",
    "    if len(articles_list) != 0:\n",
    "        now = datetime.datetime.now()\n",
    "        now_min = now.minute\n",
    "        \n",
    "        # Instanciando Pipeline\n",
    "        pipeline_data = Data_Pipeline_API()\n",
    "\n",
    "        # Carregando dados da API\n",
    "        pipeline_data.load_api(articles_list)\n",
    "\n",
    "        # Salvando em Parquet\n",
    "        pipeline_data.save_data(\"raw_data\")\n",
    "\n",
    "        print(f\"raw_data atualizado em {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        if now_min % 30 == 0:\n",
    "\n",
    "            # Importando DataFrame\n",
    "            pipeline_data.load_parquet(\"raw_data\")\n",
    "\n",
    "            # Limpeza de dados\n",
    "            pipeline_data.data_cleaning()\n",
    "\n",
    "            # Salvando Parquet\n",
    "            pipeline_data.save_data(\"cleaned_data\")\n",
    "\n",
    "            print(f\"cleaned_data atualizado em {now.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2273008145844138,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
